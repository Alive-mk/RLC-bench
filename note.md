**RLC-Bench: When to Retrieve and When to Read Long? A Quantitative Study of Accuracy–Latency–Cost**

**RAG vs Long-Context: Practical Trade-offs on Multi-Hop QA**

**Context Budgeting for LLMs: Top-k Retrieval or Long-Context Packing?**

 这个项目就是用**同一套数据**，把两种“把知识喂给大模型”的方法——**RAG（检索增强）和长上下文直塞（Long-Context）**——放到同一跑道上，**量化比较它们的准确率与耗时**，帮你决定在你的场景里该选哪条路线。

# 在干什么（What）

- 任务：多跳问答（HotpotQA 子集）。
- 两种方法：
  - **RAG**：先用向量检索挑出 **Top-K** 段落，再把这几段拼进提示里问模型。
  - **长上下文（LC）**：把能塞的相关段落尽量**一次性长文拼接**给模型。
- 度量：对每道题记录 **EM/F1（准确性）** 和 **推理延迟（ms）**，最后输出汇总表 `summary.csv`。
- 复现：固定依赖与脚本；结果逐样本落盘到 `runs/<时间戳>/*.jsonl`。

# 目的（Why）

1. **做决策**：给出“**准确率—延迟（—成本）**”的量化权衡，指导你在产品/论文里**选 RAG 还是 LC**，或两者混用的阈值。
2. **打样本**：一周内拿到第一张基准表，后续很容易扩展到**更真实的全局文档库、更多数据集**，形成论文/报告的实验骨架。
3. **工程可落地**：代码与 Docker 都给了，换模型/改参数即可在你自己的知识库上复跑。

# 你会拿到的产出（Outputs）

- `summary.csv`：两行（rag / lc）的 **EM、F1、平均延迟**；
- `rag.jsonl` / `lc.jsonl`：逐样本记录（预测、金标、延迟、上下文长度等）；
- （可选）把 `k=1/3/5`、`max_chars=6k/12k/24k` 跑成**曲线**，直观看出“多拿上下文”带来的收益与代价。

# 设计取舍（Assumptions）

- 入门版先在每题自带候选段落里检索（**闭集**、最稳），方便你先跑通；
- 下一步可扩展到**全局索引（如 Wikipedia/企业文档）**，更贴近真实检索场景；
- 成本统计（tokens）留作加分项，便于把结果转化为**费用评估**。

# 成功标准（Definition of Done）

- 跑完 100 条样本，得到一张表；通常会看到：
  - **LC** → 可能略高的 EM/F1，但**延迟更高**；
  - **RAG** → 延迟更低，对 **k** 较敏感。
- 能据此给出一句**选择建议**（例如：“我们领域里，k=3 的 RAG 已经接近 LC，但速度快 40%+”）。

如果你 ok，我们下一步就按这目标跑“第一轮 100 条”，我再教你把结果画成一张对比图，顺便做两点小消融（k 与 max_chars）。